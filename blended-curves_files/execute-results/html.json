{
  "hash": "01d7c7c8e54eab85dc5935838cd84a36",
  "result": {
    "markdown": "---\ntitle: \"Blended curves\"\n---\n\n\n## Background\n\nWe now present a novel approach to alleviate the problem of survival extrapolation with heavily censored data from clinical trials. The main idea is to mix a flexible model (e.g., Cox semiparametric) to fit as well as possible the observed data and a parametric model encoding assumptions on the expected behavior of underlying long-term survival. The two are ''blended'' into a single survival curve that is identical with the Cox model over the range of observed times and gradually approaching the parametric model over the extrapolation period based on a weight function. The weight function regulates the way two survival curves are blended, determining how the internal and external sources contribute to the estimated survival over time.\n\n## R Examples\n\nWe need to have the `blendR` package installed to run this example. This is currently available on GitHub.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndevtools::install_github(\"StatisticsHealthEconomics/blendR\")\n```\n:::\n\n\nIn the first example we will use the `survHE` and `INLA` packages to fit the external and observed data models, respectively, so attach these packages.\n\n\n::: {.cell}\n\n:::\n\n\nWe will use the data set available within `blendR` and so load data in to the current environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"TA174_FCR\", package = \"blendR\")\nhead(dat_FCR)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 Ã— 5\n  patid treat death death_t death_ty\n  <int> <int> <int>   <dbl>    <dbl>\n1     1     1     0  32       2.67  \n2     2     1     0  30.6     2.55  \n3     3     1     0  28       2.33  \n4     8     1     0  30       2.5   \n5    10     1     1   0.458   0.0382\n6    11     1     1   1.57    0.131 \n```\n:::\n:::\n\n\nFit to the observed data uinsg `INLA` to obtain the survival object. `blendR` has a helper function to do this for a piece-wise exponential distribution.\nThe `cutpoints` argument determines where the points on the survival curve are between which the hazard is constant i.e. an exponential curve.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_Surv <- blendR::surv_est_inla(data = dat_FCR,\n                                  cutpoints = seq(0, 180, by = 5))\n```\n:::\n\n\nSimilarly, we fit the external estimate but first we need to create a synthetic data set consistent with expert judgment.\nThis can be elicited ad-hoc or formally and the process of doing so is a field in itself. Once the values have been elicited then `blendR` had a function to translate from elicited survival curve constraints to a random sample of survival times.\nIn this case we suppose that we have the information that at time 144 the probability of survival is 0.05.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_sim <- blendR:::ext_surv_sim(t_info = 144,\n                                  S_info = 0.05,\n                                  T_max = 180)\n\next_Surv <- fit.models(formula = Surv(time, event) ~ 1,\n                       data = data_sim,\n                       distr = \"gompertz\",\n                       method = \"hmc\",\n                       priors = list(gom = list(a_alpha = 0.1,\n                                                b_alpha = 0.1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'Gompertz' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 17.158 seconds (Warm-up)\nChain 1:                21.773 seconds (Sampling)\nChain 1:                38.931 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'Gompertz' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.293 seconds (Warm-up)\nChain 2:                0.2 seconds (Sampling)\nChain 2:                0.493 seconds (Total)\nChain 2: \n```\n:::\n:::\n\n\nNow we are nearly ready to fit the blended survival curve. We also need to provide the additional information of how the observed data and external curves are blended together using the beta distribution. That is, we define the blending region `min` and `max` and the parameters `alpha` and `beta`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblend_interv <- list(min = 48, max = 150)\nbeta_params <- list(alpha = 3, beta = 3)\n```\n:::\n\n\nbefore putting this all together in the `blendsurv` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nble_Surv <- blendR:::blendsurv(obs_Surv, ext_Surv, blend_interv, beta_params)\n```\n:::\n\n\nA plotting method is available for `blendR` objects so simply call the following to return the blended survival curve graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(ble_Surv)\n```\n\n::: {.cell-output-display}\n![](blended-curves_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nWe can alternatively use other survival curves and fitting function for each part of the blended curve. Here we use also `fit.model` from `survHE` instead of the `INLA` fitting function for the observed data model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_Surv2 <- fit.models(formula = Surv(death_t, death) ~ 1,\n                        data = dat_FCR,\n                        distr = \"exponential\",\n                        method = \"hmc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.447 seconds (Warm-up)\nChain 1:                0.255 seconds (Sampling)\nChain 1:                0.702 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.362 seconds (Warm-up)\nChain 2:                0.273 seconds (Sampling)\nChain 2:                0.635 seconds (Total)\nChain 2: \n```\n:::\n\n```{.r .cell-code}\next_Surv2 <- fit.models(formula = Surv(time, event) ~ 1,\n                        data = data_sim,\n                        distr = \"exponential\",\n                        method = \"hmc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.157 seconds (Warm-up)\nChain 1:                0.092 seconds (Sampling)\nChain 1:                0.249 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.143 seconds (Warm-up)\nChain 2:                0.086 seconds (Sampling)\nChain 2:                0.229 seconds (Total)\nChain 2: \n```\n:::\n\n```{.r .cell-code}\nble_Surv2 <- blendR:::blendsurv(obs_Surv2, ext_Surv2, blend_interv, beta_params)\n```\n:::\n\n\nWe can also include the original data Kaplan-Meier in the output plot by simply appending it to the basic plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# kaplan-meier\nkm <- survfit(Surv(death_t, death) ~ 1, data = dat_FCR)\n\nplot(ble_Surv2) +\n  geom_line(aes(km$time, km$surv, colour = \"Kaplan-Meier\"),\n            size = 1.25, linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](blended-curves_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "blended-curves_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}