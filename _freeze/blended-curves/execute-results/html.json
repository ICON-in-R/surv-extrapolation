{
  "hash": "1d4c6dbba439d082a945ac306e8a7bd8",
  "result": {
    "markdown": "---\ntitle: \"Blended curves\"\n---\n\n\n\n## Background\n\nWe now present a novel approach to alleviate the problem of survival extrapolation with heavily censored\ndata from clinical trials. The main idea is to mix a flexible model (e.g., Cox semiparametric) to fit as well as\npossible the observed data and a parametric model encoding assumptions on the expected behavior of underlying\nlong-term survival. The two are ‘‘blended’’ into a single survival curve that is identical with the Cox model over the\nrange of observed times and gradually approaching the parametric model over the extrapolation period based on a\nweight function. The weight function regulates the way two survival curves are blended, determining how the internal\nand external sources contribute to the estimated survival over time.\n\n\n## R examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndevtools::install_github(\"StatisticsHealthEconomics/blendR\")\n```\n:::\n\n\n\nIn the first example we will use the `survHE` and `INLA` packages to fit the external and observed data models, respectively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survHE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Rcpp\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: flexsurv\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: survival\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: dplyr\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n```{.r .cell-code}\nlibrary(INLA)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: foreach\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: parallel\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: sp\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThis is INLA_22.05.07 built 2022-05-07 09:52:03 UTC.\n - See www.r-inla.org/contact-us for how to get help.\n```\n:::\n\n```{.r .cell-code}\nlibrary(blendR)\n```\n:::\n\n\nLoad data the sample data from the `blendR` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"TA174_FCR\", package = \"blendR\")\nhead(dat_FCR)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  patid treat death death_t death_ty\n  <int> <int> <int>   <dbl>    <dbl>\n1     1     1     0  32       2.67  \n2     2     1     0  30.6     2.55  \n3     3     1     0  28       2.33  \n4     8     1     0  30       2.5   \n5    10     1     1   0.458   0.0382\n6    11     1     1   1.57    0.131 \n```\n:::\n:::\n\n\nFit to the observed data to obtain the survival object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_Surv <- blendR:::surv_est_inla(data = dat_FCR,\n                          cutpoints = seq(0, 180, by = 5))\n```\n:::\n\n\nSimilarly, fit the external estimate but first we need to create a synthetic data set consistent with expert judgment.\nIn this case we suppose that we have the information that at time 144 the probability of survival is 0.05.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_sim <- blendR:::ext_surv_sim(t_info = 144,\n                         S_info = 0.05,\n                         T_max = 180)\n\next_Surv <- fit.models(formula = Surv(time, event) ~ 1,\n                       data = data_sim,\n                       distr = \"gompertz\",\n                       method = \"hmc\",\n                       priors = list(gom = list(a_alpha = 0.1,\n                                                b_alpha = 0.1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'Gompertz' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 2.966 seconds (Warm-up)\nChain 1:                0.222 seconds (Sampling)\nChain 1:                3.188 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'Gompertz' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.172 seconds (Warm-up)\nChain 2:                0.24 seconds (Sampling)\nChain 2:                0.412 seconds (Total)\nChain 2: \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There were 3 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See\nhttps://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See\nhttps://mc-stan.org/misc/warnings.html#bfmi-low\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Examine the pairs() plot to diagnose sampling problems\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The largest R-hat is 3.66, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n```\n:::\n:::\n\n\nNow we are ready to fit the blended survival curve.\nWe also need to provide the additional information of how the observed data and external curves are blended together using the beta distribution.\nThat is, we define the blending region `min` and `max` and the parameters `alpha` and `beta`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblend_interv <- list(min = 48, max = 150)\nbeta_params <- list(alpha = 3, beta = 3)\n```\n:::\n\n\nbefore putting this all together in the `blendsurv` function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nble_Surv <- blendR:::blendsurv(obs_Surv, ext_Surv, blend_interv, beta_params)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nas(<dgCMatrix>, \"dgTMatrix\") is deprecated since Matrix 1.5-0; do as(., \"TsparseMatrix\") instead\n```\n:::\n:::\n\n\nThere is a plotting method available for `blendr` objects so simple call the following to return the blended survival curve graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(ble_Surv)\n```\n\n::: {.cell-output-display}\n![](blended-curves_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nWe can alternatively use other survival curves and fitting function for each part of the blended curve.\nHere we use `fit.model` from `survHE` instead of the `INLA` fitting function for the observed data model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_Surv2 <- fit.models(formula = Surv(death_t, death) ~ 1,\n                        data = dat_FCR,\n                        distr = \"exponential\",\n                        method = \"hmc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.419 seconds (Warm-up)\nChain 1:                0.245 seconds (Sampling)\nChain 1:                0.664 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.384 seconds (Warm-up)\nChain 2:                0.323 seconds (Sampling)\nChain 2:                0.707 seconds (Total)\nChain 2: \n```\n:::\n\n```{.r .cell-code}\next_Surv2 <- fit.models(formula = Surv(time, event) ~ 1,\n                       data = data_sim,\n                       distr = \"exponential\",\n                       method = \"hmc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.138 seconds (Warm-up)\nChain 1:                0.085 seconds (Sampling)\nChain 1:                0.223 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.131 seconds (Warm-up)\nChain 2:                0.072 seconds (Sampling)\nChain 2:                0.203 seconds (Total)\nChain 2: \n```\n:::\n\n```{.r .cell-code}\nble_Surv2 <- blendR:::blendsurv(obs_Surv2, ext_Surv2, blend_interv, beta_params)\n```\n:::\n\n\nWe can also include the original data Kaplan-Meier in the output plot by simple appending it to the basic plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# kaplan-meier\nkm <- survfit(Surv(death_t, death) ~ 1, data = dat_FCR)\n\nplot(ble_Surv2) +\n  geom_line(aes(km$time, km$surv, colour = \"Kaplan-Meier\"),\n            size = 1.25, linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](blended-curves_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "blended-curves_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}