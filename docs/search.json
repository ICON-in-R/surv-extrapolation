[
  {
    "objectID": "AIC-BIC-tests.html",
    "href": "AIC-BIC-tests.html",
    "title": "AIC/BIC tests",
    "section": "",
    "text": "Akaike’s Information Criterion (AIC) and the Bayesian Information Criterion (BIC) provide a useful statistical test of the relative fit of alternative parametric models, and they are usually available as outputs from statistical software. Further details on these are available from (Collett 2013). Measures such as the negative 2 log likelihood are only suitable for comparing nested models, whereby one model is nested within another (for example, one model adds an additional covariate compared to another model). Different parametric models which use different probability distributions cannot be nested within one another. Thus the negative 2 log likelihood test is not suitable for assessing the fit of alternative parametric models. The AIC and BIC allow a comparison of models that do not have to be nested, including a term which penalises the use of unnecessary covariates (these are penalised more highly by the BIC). Generally, it is not necessary to include covariates in survival modelling in the context of an RCT as it would be expected that any important covariates would be balanced through the process of randomisation. However, some parametric models have more parameters than others, and the AIC and BIC take account of these – for example an exponential model only has one parameter and so in comparative terms two parameter models such as the Weibull or Gompertz models are penalised. The AIC and BIC statistics therefore weigh up the improved fit of models with the potentially inefficient use of additional parameters, with the use of additional parameters penalised more highly by the BIC relative to the AIC.\nSuppose that we have a statistical model of some data. Let \\(k\\) be the number of estimated parameters in the model. Let \\(\\hat{L}\\) be the maximized value of the likelihood function for the model. Then the AIC value of the model is the following.\n\\[\nAIC = 2k - 2 \\ln({\\hat{L}})\n\\]\nGiven a set of candidate models for the data, the preferred model is the one with the minimum AIC value.\nThe BIC is formally defined as\n\\[\nBIC = k \\ln(n) - 2 \\ln(\\widehat{L})\n\\]\nwhere\n\\(\\hat{L}\\) = the maximized value of the likelihood function of the model \\(M\\), i.e. \\(\\hat{L} = p(x \\mid \\widehat{\\theta}, M)\\), where \\(\\widehat{\\theta}\\) are the parameter values that maximize the likelihood function; \\(x\\) = the observed data; \\(n\\) = the number of data points in \\(x\\), the number of observations, or equivalently, the sample size; \\(k\\) = the number of parameters estimated by the model. For example, in multiple linear regression, the estimated parameters are the intercept, the \\(q\\) slope parameters, and the constant variance of the errors; thus, \\(k = q + 2\\).\n\n\n\n\n\n\nRelative values\n\n\n\nIC values only really make sense when compared between one another i.e relatively. The absolute value is not helpful and depends on the specifics of the model and data."
  },
  {
    "objectID": "AIC-BIC-tests.html#r-example",
    "href": "AIC-BIC-tests.html#r-example",
    "title": "AIC/BIC tests",
    "section": "R example",
    "text": "R example\nCox proportional hazard model\n\nlibrary(survival)\n\ntest1 <- list(time=c(4,3,1,1,2,2,3), \n              status=c(1,1,1,0,1,1,0), \n              x=c(0,2,1,1,1,0,0), \n              sex=c(0,0,0,0,1,1,1)) \n# Fit a stratified model \nfit_ph <- coxph(Surv(time, status) ~ x + strata(sex), test1) \n\nAIC(fit_ph)\n\n[1] 8.655313\n\n##TODO: what this difference with AIC()?\nextractAIC(fit_ph)\n\n[1] 1.000000 8.655313\n\nBIC(fit_ph)\n\n[1] 8.264751\n\n\n\nParametric model with flexsurv\nThe package flexsurv computes the AIC when it fits a model. This can be accessed by `.\\(AIC\\) i.e.\n\nlibrary(flexsurv)\n\nfitg <- flexsurvreg(formula = Surv(futime, fustat) ~ 1, data = ovarian, dist = \"gengamma\")\nfitg$AIC\n\n[1] 199.8981\n\n\nFor other information criteria statistics it is straightforward to calculate these using the flexsurvreg outpuu. For simplicity we can write a function to do them all at the same time, which we shall call fitstats.flexsurvreg.\n\n\nfitstats.flexsurvreg() function\n# helper function\n# from flexsurv github\n# https://github.com/chjackson/flexsurv-dev/issues/44\nfitstats.flexsurvreg <- function(x) {\n  ll <- x$loglik\n  aic <- x$AIC\n  k <- length(x$coefficients)\n  n <- sum(x$data$m[\"(weights)\"])\n  aicc <- aic + ((2 * k) * (k + 1) / (n - k - 1))\n  bic <- - 2 * ll + (k * log(n))\n  \n  data.frame(\n   Df = k,\n    \"n2ll\" = -2 * ll,\n    AIC = aic,\n    AICc = aicc,\n    BIC = bic)\n}\n\n\nNow if we run this we obtain all of the statistics for the previous generalised gamma model fit.\n\nfitstats.flexsurvreg(fitg)\n\n  Df     n2ll      AIC    AICc      BIC\n1  3 193.8981 199.8981 200.989 203.6724\n\n\n\n\nBayesian model with survHE package\nFirst let us fit an example model using survHE.\n\nlibrary(survHE)\n\ndata(bc)\n\nmle <- fit.models(formula = Surv(recyrs,censrec) ~ group,\n                  data = bc,\n                  distr = \"exp\",\n                  method = \"mle\")\n\nNow, the print method for class survHE returns several model fitting summaries.\n\nprint(mle)\n\n\nModel fit for the Exponential model, obtained using Flexsurvreg \n(Maximum Likelihood Estimate). Running time: 0.020 seconds\n\n                 mean         se      L95%      U95%\nrate        0.0603838 0.00845542 0.0458911 0.0794534\ngroupMedium 0.8180219 0.17122084 0.4824352 1.1536086\ngroupPoor   1.5375232 0.16280169 1.2184378 1.8566087\n\nModel fitting summaries\nAkaike Information Criterion (AIC)....: 1668.212\nBayesian Information Criterion (BIC)..: 1681.805\n\n\nIf we wished to access the values directly, perhaps to use in our own code, then we can use\n\nmle$model.fitting\n\n$aic\n[1] 1668.212\n\n$bic\n[1] 1681.805\n\n$dic\nNULL\n\n\nFurther, if we were to fit several different models to compare the IC statistics, which is really the main point of doing it, then survHE also has some nice plotting functions.\n\nmle_multi <- fit.models(formula = Surv(recyrs,censrec) ~ group,\n                  data = bc,\n                  distr = c(\"exp\", \"weibull\", \"gompertz\", \"lognormal\", \"loglogistic\"),\n                  method = \"mle\")\n\nmodel.fit.plot(mle_multi)\n\n\n\nmodel.fit.plot(mle_multi, type = \"BIC\")\n\n\n\nmodel.fit.plot(mle_multi, scale = \"relative\")"
  },
  {
    "objectID": "assess-transformed-km.html",
    "href": "assess-transformed-km.html",
    "title": "Assessing model assumptions using transformed hazard plots",
    "section": "",
    "text": "Prior to fitting a model based on an assumed parametric form for the hazard function, a preliminary study of the validity of this assumption should be carried-out.\nLet us compare the survivor function for the data with that from a chosen model. To do this we will transform the survivor function to produce a plot that should give a straight line if the assumed model is appropriate.\nFor the Weibull, twice taking logs of the survivor function with scale parameter \\(\\lambda\\) and shape parameter \\(\\gamma\\)\n\\[\nlog(-log S(t)) = log \\lambda + \\gamma log t\n\\]\nA plot of \\(log(-log S(t))\\) against \\(log(t)\\) would give an approximately straight line if the Weibull assumption is reasonable. The plot could also be used to give a rough estimate of the parameters.\nSimilarly, for the log-logistic distribution\n\\[\nlog S(t)/(1 - S(t)) = \\theta - \\kappa log t\n\\]\nFor the log-normal distribution\n\\[\n\\Phi^{-1} (1 - S(t)) = (log t - \\mu) / \\sigma\n\\] The slope and intercept of this line provide estimates of \\(\\sigma^{-1}\\) and \\(-\\mu/\\sigma\\), respectively.\nWe can also check the assumption made with using the Cox regression model of proportional hazards by inspecting the log-cumulative hazard plot.\n\\[\nlog H_i(t) = \\beta x_i + log H_0(t)\n\\]\nThe transformed curves for different values of the explanatory variables will be parallel if PH holds.\nSee Collett (2013) for more details."
  },
  {
    "objectID": "assess-transformed-km.html#r-examples",
    "href": "assess-transformed-km.html#r-examples",
    "title": "Assessing model assumptions using transformed hazard plots",
    "section": "R examples",
    "text": "R examples\nThe package commonly used for survival analyses in R is the survival package (Therneau T 2021). We will begin by repeating an example from the survival help documentation.\nThis uses their reliability data. Firstly a little data manipulation is done before we plot the cumulative hazard plot against time using the in-built survival package plotting method with the cumhaz=TRUE argument.\n\nlibrary(survival)\n\ndata(\"reliability\", package = \"survival\")\n\nvdata <- with(valveSeat, data.frame(id = id, time2 = time, status = status))\nfirst <- !duplicated(vdata$id)\nvdata$time1 <- ifelse(first, 0, c(0, vdata$time[-nrow(vdata)]))\ndouble <- which(vdata$time1 == vdata$time2)\nvdata$time1[double] <- vdata$time1[double] - 0.01\nvdata$time2[double - 1] <- vdata$time1[double]\nvdata[1:7, c(\"id\", \"time1\", \"time2\", \"status\")]\n\n   id  time1  time2 status\n1 251   0.00 761.00      0\n2 252   0.00 759.00      0\n3 327   0.00  98.00      1\n4 327  98.00 667.00      0\n5 328   0.00 326.00      1\n6 328 326.00 652.99      1\n7 328 652.99 653.00      1\n\n\n\nfit <- survfit(Surv(time1, time2, status) ~ 1, data = vdata, id = id)\nplot(fit, cumhaz = TRUE, xlab = \"Days\", ylab = \"Cumulative hazard\")\n\n\n\n\nWe can plot the log-cumulative hazard against log-time by simply plotting the survfit output values directly by specifying the x and y data explicitly.\n\nplot(log(fit$time), log(fit$cumhaz), xlab = \"log-Days\", ylab = \"Log-cumulative hazard\", type = \"l\")\n\n\n\n\nFor the following, the latest development version fo the survHE package (Baio 2020) contains all of the functions that we will need. We can obtain this from GitHub with the following.\n\ndevtools::install_github(\"giabaio/survHE\", ref = \"devel\")\n\nIn particular, we will need the plot_transformed_km function.\n\n\nplot_transformed_km() function\nplot_transformed_km <- function(fit, mod = 1, add_legend = FALSE,\n                                graph = \"base\", ...) {\n  \n  dots <- list(...)\n  \n  graph <- match.arg(graph, c(\"base\", \"ggplot2\"))\n  \n  if (length(mod) != 1)\n    stop(\"Please provide at most one model index.\")\n  \n  if (is.numeric(mod) && !mod <= length(fit$models))\n    stop(\"More model names provided than available in list of model fits provided.\")\n  \n  if (is.character(mod) && !mod %in% names(fit$models))\n    stop(\"Model name not available in list of model fits provided.\")\n  \n  dist <- get_distribution(fit, mod)\n  \n  distn_names <- list(\n    \"exp\" = c(\"exp\", \"exponential\"),\n    \"weibull\" = c(\"weibull\", \"weibull.quiet\", \"weibullaf\", \"weibullph\"),\n    \"loglogistic\" = c(\"llogis\", \"loglogistic\"),\n    \"lognormal\" = c(\"lognormal\", \"lnorm\"),\n    \"gompertz\" = \"gompertz\")\n  \n  if (!dist %in% unname(unlist(distn_names)))\n    stop(\"Distribution not available.\")\n  \n  fit_km <- fit$misc$km\n  \n  n_strata <- length(fit_km$strata)\n  \n  if (n_strata == 0 || n_strata == 1) {\n    fit_km$strata <- c(\"group\" = length(fit_km$time))\n  }\n  \n  model_strata <- rep(x = names(fit_km$strata),\n                      times = fit_km$strata)\n  \n  times <- split(fit_km$time, model_strata)\n  survs <- split(fit_km$surv, model_strata)\n  \n  params <- list()\n  \n  if (dist %in% distn_names[[\"exp\"]]) {\n    params <- list(\n      FUN = \"lines\",\n      xlab = \"time\",\n      ylab = \"-log(S(t))\",\n      main = \"Exponential distributional assumption\",\n      x = times,\n      y = lapply(survs, function(x) -log(x)),\n      lty = 1:n_strata,\n      col = 1:n_strata,\n      type = \"l\")\n  }\n  \n  if (dist %in% distn_names[[\"weibull\"]]) {\n    params <- list(\n      FUN = \"lines\",\n      xlab = \"log(time)\",\n      ylab = \"log(-log(S(t))) i.e. log cumulative hazard\",\n      main = \"Weibull distributional assumption\",\n      x = lapply(times, log),\n      y = lapply(survs, function(x) log(-log(x))),\n      lty = 1:n_strata,\n      col = 1:n_strata,\n      type = \"l\")\n  }\n  \n  if (dist %in% distn_names[[\"loglogistic\"]]) {\n    params <- list(\n      FUN = \"lines\",\n      xlab = \"time\",\n      ylab = \"log(S(t)/(1-S(t)))\",\n      main = \"log-Logistic distributional assumption\",\n      x = lapply(times, log),\n      y = lapply(survs, function(x) log(x/(1 - x))),\n      lty = 1:n_strata,\n      col = 1:n_strata,\n      type = \"l\")\n  }\n  \n  if (dist %in% distn_names[[\"lognormal\"]]) {\n    params <- list(\n      FUN = \"lines\",\n      xlab = \"log(time)\",\n      ylab = expression(Phi^-1 ~ (1 - S(t))),\n      main = \"Log-normal distributional assumption\",\n      x = lapply(times, log),\n      y = lapply(survs, function(x) qnorm(1 - x)),\n      lty = 1:n_strata,\n      col = 1:n_strata,\n      type = \"l\")\n  }\n  \n  default_pars <- list(\n    x = NULL,\n    type = \"n\",\n    axes = FALSE,\n    xlab = params$xlab,\n    ylab = params$ylab,\n    main = params$main,\n    xlim = range(pretty(unlist(params$x))),\n    ylim = range(pretty(unlist(params$y))))\n  \n  setup_pars <- modifyList(\n    default_pars, dots[names(default_pars)])\n  \n  if (graph == \"base\") {\n    \n    # empty plot\n    do.call(plot, setup_pars)\n    \n    axis(1); axis(2)\n    \n    # plot lines\n    do.call(mapply, modifyList(params, dots))\n    \n    if (isTRUE(add_legend)) {\n      legend(\"topright\", names(survs), col = params$col,\n             lty = params$lty, bty = \"n\")\n    }\n  }\n  \n  if (graph == \"ggplot2\") {\n    \n    if (!add_legend) {\n      pos.legend <- \"none\"\n    } else {\n      pos.legend <- \"right\"}\n    \n    ggdata <- \n      data.frame(time = unlist(params$x),\n                 y = unlist(params$y)) |>\n      tibble::rownames_to_column(\"Group\") |> \n      mutate(Group = gsub(\"\\\\d+\", \"\", Group))\n    \n    p <- \n      ggplot(ggdata, aes(x = .data$time, y = .data$y,\n                         group = .data$Group, col = .data$Group)) +\n      geom_line() +\n      do.call(labs,\n              list(title = setup_pars$main,\n                   x = setup_pars$xlab,\n                   y = setup_pars$ylab)) +\n      theme_bw() +\n      theme(legend.position = pos.legend)\n    \n    print(p)\n  }\n  \n  invisible(params)\n}\n\nget_distribution <- function(fit, mod) {\n    m <- fit$models[[mod]]\n    tolower(ifelse(fit$method == \"hmc\", m@model_name, m$dlist$name))\n}\n\n\nNow we can repeat the above analysis using the plot_transformed_km function. By setting distr = \"exp\" the cumulative hazard plot is returned.\n\nlibrary(survHE)\n\nfit_exp <- survHE::fit.models(Surv(time1, time2, status) ~ 1,\n                              data = vdata, distr = \"exp\", method = \"mle\")\nplot_transformed_km(fit_exp)\n\n\n\n\nSetting distr = \"weibull\" then we get the log-cumulative hazard against log-time plot.\n\nfit_wei <- survHE::fit.models(Surv(time1, time2, status) ~ 1,\n                              data = vdata, distr = \"weibull\", method = \"mle\")\nplot_transformed_km(fit_wei)\n\n\n\n\nThe plot_transformed_km also provides plots for log-normal and log-logistic distribution assumptions with the corresponding transformation to the survival data.\n\nUsing flexsurv\nFurther, we could use the flexsurv package (Jackson 2016). This package contains lots of functions for a range of survival distributions.\nThe cumulative hazard can be plotted with the flexsurv plotting method with argument type = \"cumhaz\". The Kaplan-Meier is also overlaid by the model fit.\n\nlibrary(\"flexsurv\")\n\nfs1 <- flexsurvreg(Surv(time1, time2, status) ~ 1, data = vdata, dist = \"exp\")\nplot(fs1, type = \"cumhaz\")\n\n\n\nfs2 <- flexsurvreg(Surv(time1, time2, status) ~ 1, data = vdata, dist = \"weibull\")\nplot(fs2, type = \"cumhaz\")\n\n\n\n\n\n\nUsing survHE\n\nlibrary(\"survHE\")\n\nfs1 <- fit.models(Surv(time1, time2, status) ~ 1, data = vdata, dist = \"exp\")\nplot(fs1, type = \"cumhaz\")\n\n\n\nfs2 <- fit.models(Surv(time1, time2, status) ~ 1, data = vdata, dist = \"weibull\")\nplot(fs2, type = \"cumhaz\")"
  },
  {
    "objectID": "basic-visual-inspection.html",
    "href": "basic-visual-inspection.html",
    "title": "Basic visual inspection",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blended-curves.html",
    "href": "blended-curves.html",
    "title": "Blended survival curves",
    "section": "",
    "text": "We now present a novel approach to alleviate the problem of survival extrapolation with heavily censored data from clinical trials. The main idea is to mix a flexible model (e.g., Cox semiparametric) to fit as well as possible the observed data and a parametric model encoding assumptions on the expected behaviour of underlying long-term survival. The two are ‘’blended’’ into a single survival curve that is identical with the Cox model over the range of observed times and gradually approaching the parametric model over the extrapolation period based on a weight function. The weight function regulates the way two survival curves are blended, determining how the internal and external sources contribute to the estimated survival over time. Further details can be found in (Che, Green, and Baio 2022).\n\n\n\n\n\n\nShiny app\n\n\n\nThere is an RShiny version of the blendR package for using blended curves interactively in the browser. See here."
  },
  {
    "objectID": "blended-curves.html#r-examples",
    "href": "blended-curves.html#r-examples",
    "title": "Blended survival curves",
    "section": "R Examples",
    "text": "R Examples\nWe need to have the blendR package installed to run this example. This is currently available on GitHub.\n\ndevtools::install_github(\"StatisticsHealthEconomics/blendR\")\n\nIn the first example we will use the survHE and INLA packages to fit the external and observed data models, respectively. If the survHE version for doing HMC is missing then install this.\n\nremotes::install_github('giabaio/survHE', ref='hmc')\n\nAttach these packages.\n\n\n\nWe will use the data set available within blendR and so load data in to the current environment.\n\ndata(\"TA174_FCR\", package = \"blendR\")\nhead(dat_FCR)\n\n# A tibble: 6 × 5\n  patid treat death death_t death_ty\n  <int> <int> <int>   <dbl>    <dbl>\n1     1     1     0  32       2.67  \n2     2     1     0  30.6     2.55  \n3     3     1     0  28       2.33  \n4     8     1     0  30       2.5   \n5    10     1     1   0.458   0.0382\n6    11     1     1   1.57    0.131 \n\n\nFit to the observed data uinsg INLA to obtain the survival object. blendR has a helper function to do this for a piece-wise exponential distribution. The cutpoints argument determines where the points on the survival curve are between which the hazard is constant i.e. an exponential curve.\n\nobs_Surv <- blendR::surv_est_inla(data = dat_FCR,\n                                  cutpoints = seq(0, 180, by = 5))\n\nSimilarly, we fit the external estimate but first we need to create a synthetic data set consistent with expert judgment. This can be elicited ad-hoc or formally and the process of doing so is a field in itself. Once the values have been elicited then blendR had a function to translate from elicited survival curve constraints to a random sample of survival times. In this case we suppose that we have the information that at time 144 the probability of survival is 0.05.\n\ndata_sim <- blendR::ext_surv_sim(t_info = 144,\n                                  S_info = 0.05,\n                                  T_max = 180)\n\next_Surv <- fit.models(formula = Surv(time, event) ~ 1,\n                       data = data_sim,\n                       distr = \"gompertz\",\n                       method = \"hmc\",\n                       priors = list(gom = list(a_alpha = 0.1,\n                                                b_alpha = 0.1)))\n\n\nSAMPLING FOR MODEL 'Gompertz' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.258 seconds (Warm-up)\nChain 1:                0.22 seconds (Sampling)\nChain 1:                0.478 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'Gompertz' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.233 seconds (Warm-up)\nChain 2:                0.212 seconds (Sampling)\nChain 2:                0.445 seconds (Total)\nChain 2: \n\n\nNow we are nearly ready to fit the blended survival curve. We also need to provide the additional information of how the observed data and external curves are blended together using the beta distribution. That is, we define the blending region min and max and the parameters alpha and beta.\n\nblend_interv <- list(min = 48, max = 150)\nbeta_params <- list(alpha = 3, beta = 3)\n\nbefore putting this all together in the blendsurv function.\n\nble_Surv <- blendR::blendsurv(obs_Surv, ext_Surv, blend_interv, beta_params)\n\nA plotting method is available for blendR objects so simply call the following to return the blended survival curve graph.\n\nplot(ble_Surv)\n\n\n\n\nWe can alternatively use other survival curves and fitting function for each part of the blended curve. Here we use also fit.model from survHE instead of the INLA fitting function for the observed data model.\n\nobs_Surv2 <- fit.models(formula = Surv(death_t, death) ~ 1,\n                        data = dat_FCR,\n                        distr = \"exponential\",\n                        method = \"hmc\")\n\n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.405 seconds (Warm-up)\nChain 1:                0.278 seconds (Sampling)\nChain 1:                0.683 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.462 seconds (Warm-up)\nChain 2:                0.26 seconds (Sampling)\nChain 2:                0.722 seconds (Total)\nChain 2: \n\next_Surv2 <- fit.models(formula = Surv(time, event) ~ 1,\n                        data = data_sim,\n                        distr = \"exponential\",\n                        method = \"hmc\")\n\n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.183 seconds (Warm-up)\nChain 1:                0.087 seconds (Sampling)\nChain 1:                0.27 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.163 seconds (Warm-up)\nChain 2:                0.09 seconds (Sampling)\nChain 2:                0.253 seconds (Total)\nChain 2: \n\nble_Surv2 <- blendR::blendsurv(obs_Surv2, ext_Surv2, blend_interv, beta_params)\n\nWe can also include the original data Kaplan-Meier in the output plot by simply appending it to the basic plot.\n\n# kaplan-meier\nkm <- survfit(Surv(death_t, death) ~ 1, data = dat_FCR)\n\nplot(ble_Surv2) +\n  geom_line(aes(km$time, km$surv, colour = \"Kaplan-Meier\"),\n            size = 1.25, linetype = \"dashed\")"
  },
  {
    "objectID": "external-data.html",
    "href": "external-data.html",
    "title": "External data",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Survical curve extrapolation",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro-to-R.html",
    "href": "intro-to-R.html",
    "title": "Introduction to R",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "\\[\n\\mbox{E}[T \\mid \\boldsymbol{\\theta}]= \\int_0^\\infty S(t)dt.\n\\]\nThis implies that the expected (i.e. mean) survival time can be computed as the “area under the survival curve”. More importantly, this clarifies why applications of survival modelling in HTA requires a full extrapolation of the survival curve: the focus of the economic analysis is on the mean survival time and in order to obtain it, we need to be able to derive the full survival curve in the range of times \\(t = 0, \\ldots, \\infty\\).\nA challenge with estimating lifetime survival based on trial data is that we can only observe death times \\(T\\) that are shorter than the follow-up time of the trial. For people still alive at the end of the trial, the survival times are right-censored. To estimate \\(E(T)\\), we need to know the distribution of \\(T\\) over all plausible values of \\(T\\)."
  },
  {
    "objectID": "medians.html",
    "href": "medians.html",
    "title": "Median survival time",
    "section": "",
    "text": "The median survival time is the length of time from either the date of diagnosis or the start of treatment for a disease, such as cancer, that half of the patients in a group of patients diagnosed with the disease are still alive. In a clinical trial, measuring the median overall survival is one way to see how well a new treatment works. Also called median survival.\n\n\n\n\n\n\nTip\n\n\n\nThe median is useful but it is the expected or mean survival time that is of particular interest for HTA."
  },
  {
    "objectID": "medians.html#r-examples",
    "href": "medians.html#r-examples",
    "title": "Median survival time",
    "section": "R Examples",
    "text": "R Examples\nIn this example we will see a comparison of survival probabilities at given landmark times as well as the comparison of observed (i.e. based on Kaplan-Meier) and predicted medians (using the respective formula to calculate the median for each distribution) based on fitted models for each of the 6 main distributions we consider.\nThe summary method for a survHE object from the survHE package returns mean survival times, including the median mean survival time (not be be confused with the mean median survival time!). For an exponential model fit with no covariates,\n\nlibrary(survHE)\n\ndata(bc)\n\nmle <- fit.models(formula = Surv(recyrs, censrec) ~ 1,\n                  data = bc,\n                  distr = \"exp\",\n                  method = \"mle\")\n\nsummary(mle)\n\n\nEstimated average survival time distribution* \n     mean        sd    2.5%   median    97.5%\n 4.517346 0.1119987 4.27808 4.524495 4.725423\n\n*Computed over the range: [0.02192-7.28493] using 1000 simulations.\nNB: Check that the survival curves tend to 0 over this range!\n\n\nNote that this is calculated over a closed range and not the entire time line.\nWe can compare these parametric estimate with the median survival time from the Kaplan-Meier. This is available from the survHE output in misc$km and the equation\n\\[\n\\min \\{t : \\hat{S}(t) < 0.5 \\}\n\\]\n\nt_med <- min(mle$misc$km$time[mle$misc$km$surv < 0.5])\nt_low <- min(mle$misc$km$time[mle$misc$km$lower < 0.5])\nt_upp <- min(mle$misc$km$time[mle$misc$km$upper < 0.5])\n\nt_med\n\n[1] 4.950685\n\n\nThere is clearly some repitition here so we can simplify as follows.\n\nsurv_median <- function(S, sname) {\n  min(S[[\"time\"]][S[[sname]] < 0.5])\n}\n\nKM <- mle$misc$km\n\nsurv_median(KM, \"surv\")\n\n[1] 4.950685\n\nsurv_median(KM, \"lower\")\n\n[1] 4.347945\n\nsurv_median(KM, \"upper\")\n\n[1] 5.561644\n\n\nPlotting the Kaplan-Meier we can indicate these median times.\n\nsurvfit(Surv(recyrs, censrec) ~ 1, data = bc) |> \n  plot()\nabline(h = 0.5)\nabline(v = c(t_low, t_med, t_upp), lty = c(2,1,2))\n\n\n\n\n\nDirect estimates\nIf we denote the median with \\(t_{50}\\) then to calculate the medians ourselves we can take the fitted coefficient value from the fit.model output and use an inverese of the survival function. In the case of the exponential distribution this is\n\\[\nt_{50} = -\\log (0.5)/\\lambda\n\\]\n\nrate <- mle$models$Exponential$coefficients\nexp(rate)\n\n[1] 0.1414765\n\n# closed form\nmeantime <- -log(0.5)/exp(rate)\nmeantime\n\n[1] 4.899379\n\n\nThe log-logistic distribution has CDF\n\\[\n\\frac{1}{(1 + (t/\\alpha)^{\\beta})^2}\n\\]\nWhich leads to the median \\(t_{50} = \\alpha\\), i.e. simply the shape parameter.\nSimilarly, the Gompertz distribution median is\n\\[\n(1/b) \\log[(-1/\\eta) \\log(0.5) + 1]\n\\]\nThe Weibull distribution median is\n\\[\n\\lambda [- \\log(0.5)]^{1/k}\n\\]\nThe log-normal distribution median is\n\\[\n\\exp(\\mu)\n\\]\nThe gamma distribution has no simple closed form formula for the median.\n\n\nSimulation-based estimates\nNote that the parameter returned from fit.model is the log of the rate. More generally, we can simulate (multiple) survival curves from the coefficient posterior and estimate the median for each of these. So, sample from the posterior using make.surv() from the survHE package to obtain output for the single curve case as follows.\n\nsurv_exp <- make.surv(mle)\n\nThe sampled survival curves from make.surv() have slightly different names so let us redefine the median function and then extract the median times.\n\nsurv_median <- function(S, sname) {\n  min(S[[\"t\"]][S[[sname]] < 0.5])\n}\n\nsurv <- surv_exp$S[[1]]\n\nsurv_median(surv, \"S\")\n\n[1] 4.893151\n\n\nIt follows that we can do something similar for multiple simulations to obtain uncertainty bounds. Repeating the above but for 100 simulations,\n\nsim100 <- make.surv(mle, nsim = 100)\n\ndirect estimates are\n\nrtimes <- -log(0.5)/unlist(sim100$sim)\nrtimes\n\n  [1] 5.030977 4.860997 5.252989 4.518656 4.799633 5.155153 5.058485 5.045317\n  [9] 4.674038 4.590355 5.082505 4.817995 5.004585 4.345927 5.234568 4.935574\n [17] 5.289443 4.937837 5.136835 5.397261 4.991506 4.687472 4.441882 5.297241\n [25] 4.979869 4.921964 5.291105 4.810240 4.932064 4.582963 4.832290 4.782677\n [33] 5.181210 4.676762 4.737412 4.864239 4.748428 4.943325 4.486343 5.128842\n [41] 5.043965 5.019482 5.262224 4.931228 5.050934 5.195667 5.225034 4.822411\n [49] 4.898454 4.915956 4.891791 4.525586 4.322594 4.706158 4.779160 4.541179\n [57] 4.455202 4.608087 4.958289 4.778545 4.941223 4.493003 5.070293 4.827053\n [65] 4.877539 4.613147 4.916051 5.140611 4.687538 4.914277 5.097975 4.678640\n [73] 4.889496 4.663478 4.903244 4.632844 4.085452 4.907163 4.561742 4.876387\n [81] 4.538998 5.104637 5.261416 4.834354 5.296088 4.794037 5.092592 4.717819\n [89] 5.300351 4.884407 4.896563 5.158070 4.990269 4.862905 4.982634 4.404314\n [97] 5.077803 5.287934 5.649222 5.019902\n\n\nand simulated estimates\n\nsurv <- sim100$S[[1]]\n\nt_S <- surv_median(surv, \"S\")\nt_low <- surv_median(surv, \"low\")\nt_upp <- surv_median(surv, \"upp\")\n\nt_S\n\n[1] 4.893151\n\n\nThe plot with all samples of medians is,\n\nplot(mle) + \n  geom_vline(xintercept = rtimes, alpha = 0.1, col = \"darkgrey\", size = 2) +\n  geom_vline(xintercept = meantime) +\n  geom_vline(xintercept = t_low, linetype = 2) +\n  geom_vline(xintercept = t_upp, linetype = 2)\n\n\n\n\n\n\nMultiple distributions\nIn the same way as for a single distribution, we can extend the analysis for multiple distributions at the same time. We show this for exponential and log-logistic distributions. First, fit the models and show the survival curves.\n\nfit2 <- fit.models(formula = Surv(recyrs, censrec) ~ 1,\n                   data = bc,\n                   dist = c(\"exp\", \"loglogistic\"),\n                   method = \"mle\")\n\nplot(fit2)\n\n\n\n\nThen, sample the survival curves and rearrange so that its straightforward to use the data in the same way as above.\n\nNSIM <- 100\nsim <- list()\nsim[[1]] <- make.surv(fit2, mod = 1, nsim = NSIM)\nsim[[2]] <- make.surv(fit2, mod = 2, nsim = NSIM)\n\nsim <- purrr::transpose(sim)\n\nWe can then get the direct estimates,\n\nrtimes <- list()\nrtimes[[1]] <- -log(0.5)/sim$sim[[1]][[1]][, \"rate\"]\nrtimes[[2]] <- sim$sim[[2]][[1]][, \"scale\"]\n\nrtimes\n\n[[1]]\n  [1] 4.785691 4.773674 4.724659 4.169345 4.872511 4.646455 4.887068 4.809442\n  [9] 4.803092 5.132489 4.947968 4.806153 4.940416 4.788849 4.600579 4.797242\n [17] 4.752358 5.046295 5.056700 4.930276 4.718069 4.754700 4.811955 4.786615\n [25] 4.420308 5.279533 4.965886 5.085997 5.303584 4.781336 4.535886 4.459000\n [33] 4.940053 4.626905 4.994843 5.284709 4.620484 5.249167 5.610395 4.282181\n [41] 4.878309 5.084752 4.971148 4.679309 5.334013 4.789652 4.387141 4.536155\n [49] 5.284586 4.559396 4.392254 5.134116 5.050669 4.557677 5.055830 4.967826\n [57] 4.791461 4.893359 4.651476 4.781939 5.300166 5.219971 5.145551 5.045186\n [65] 5.158304 4.545387 5.596427 4.560900 4.899184 4.923309 4.868401 4.902518\n [73] 4.888902 4.854930 5.358622 5.437532 5.014616 4.865152 5.379231 4.667421\n [81] 4.729015 5.090545 5.113333 4.666649 5.291319 4.920715 4.700209 5.086914\n [89] 5.255839 4.593132 4.697289 4.906682 5.063687 4.543472 5.146893 5.216049\n [97] 5.370036 5.054706 4.988012 4.988741\n\n[[2]]\n  [1] 4.686934 4.443468 4.164848 4.787431 4.429112 4.619603 4.885367 4.461403\n  [9] 4.397005 4.052862 4.277117 4.482872 4.223199 4.580381 4.186790 4.486515\n [17] 4.485053 4.717372 4.424844 4.174030 4.793633 4.615457 4.260316 4.568796\n [25] 4.433402 4.688994 4.300801 4.525906 4.142817 4.379025 4.405320 4.362275\n [33] 4.609253 4.524031 4.599876 4.576962 4.979010 4.348849 4.237348 4.509658\n [41] 4.280489 5.057910 4.257423 4.463721 4.002849 4.462267 4.325352 5.008261\n [49] 4.711156 4.269269 4.347390 5.009185 4.171342 4.296398 4.027459 4.377073\n [57] 4.911544 4.454419 4.324823 4.923518 4.658023 4.643688 4.704392 4.524859\n [65] 4.387345 4.380771 4.570652 4.540388 4.562887 4.522572 4.751014 4.570938\n [73] 4.427822 4.723791 4.122711 4.555768 4.319784 4.709845 4.121744 4.712118\n [81] 4.779979 4.679062 4.238908 4.491942 4.599347 4.705663 4.280723 4.239860\n [89] 4.499630 4.472964 4.582056 4.567836 4.480996 4.642663 4.564446 4.807256\n [97] 4.065935 4.377016 5.196162 4.504275\n\n\nand the sampled estimates,\n\n# simulated estimates\nt_S <- purrr::map_dbl(sim$S, ~ surv_median(.x[[1]], \"S\"))\nt_low <- purrr::map_dbl(sim$S, ~ surv_median(.x[[1]], \"low\"))\nt_upp <- purrr::map_dbl(sim$S, ~ surv_median(.x[[1]], \"upp\"))\n\nPlotting the two sets of medians we can see the location and spread for both distributions together.\n\nplot(fit2) + \n  geom_vline(xintercept = rtimes[[1]], alpha = 0.1, col = \"pink\", size = 2) +\n  geom_vline(xintercept = rtimes[[2]], alpha = 0.1, col = \"lightblue\", size = 2) +\n  geom_vline(xintercept = t_S[[1]]) +\n  geom_vline(xintercept = t_low[[1]], linetype = 2) +\n  geom_vline(xintercept = t_upp[[1]], linetype = 2) +\n  geom_vline(xintercept = t_S[[2]]) +\n  geom_vline(xintercept = t_low[[2]], linetype = 3) +\n  geom_vline(xintercept = t_upp[[2]], linetype = 3)\n\n\n\n\n\n\nMultiple percentiles\nA general formula for the \\(p\\)th sample percentile of the survival time distribution is computed as\n\\[\nt_p = \\frac{1}{2} \\left( \\min\\{t:1−\\hat{S}(t) ≥ p\\} + \\max\\{t:1−\\hat{S}(t) ≤ p\\} \\right)\n\\]\nSo, analogous to the median only example above, let us fit an exponential distribution.\n\nmle <- fit.models(formula = Surv(recyrs, censrec) ~ 1,\n                  data = bc,\n                  distr = \"exp\",\n                  method = \"mle\")\n\nsurv <- make.surv(mle, nsim = NSIM)\n\nWe can extend the surv_median function by creating a function factory which we can use to create equivalent functions for different percentiles.\n\nsurv_percentile <- function(p) {\n  force(p)\n  function(S, sname)\n    min(S[[\"t\"]][S[[sname]] < p])\n}\n\nsurv_median <- surv_percentile(0.5)\nsurv_median(surv$S[[1]], \"S\")\n\n[1] 4.947945\n\n\nNow we can automatically create functions for all the percentiles of interest by mapping over the vector of probabilities, which returns a list of functions.\n\nprctile <- c(\"97.5\" = 0.975, \"75\" = 0.75, \"50\" = 0.5, \"25\" = 0.25, \"2.5\" = 0.025)\n\np_fns <- purrr::map(prctile, surv_percentile)\n\nhead(p_fns)\n\n$`97.5`\nfunction(S, sname)\n    min(S[[\"t\"]][S[[sname]] < p])\n<bytecode: 0x00000252edba80c8>\n<environment: 0x00000252edf48270>\n\n$`75`\nfunction(S, sname)\n    min(S[[\"t\"]][S[[sname]] < p])\n<bytecode: 0x00000252edba80c8>\n<environment: 0x00000252edb7e6f0>\n\n$`50`\nfunction(S, sname)\n    min(S[[\"t\"]][S[[sname]] < p])\n<bytecode: 0x00000252edba80c8>\n<environment: 0x00000252edb7e9c8>\n\n$`25`\nfunction(S, sname)\n    min(S[[\"t\"]][S[[sname]] < p])\n<bytecode: 0x00000252edba80c8>\n<environment: 0x00000252edb7eca0>\n\n$`2.5`\nfunction(S, sname)\n    min(S[[\"t\"]][S[[sname]] < p])\n<bytecode: 0x00000252edba80c8>\n<environment: 0x00000252edb7ef78>\n\n\nEquivalent to what we did with just the median function we can do the same with the list of percentile functions.\n\nsimdat <- surv$S[[1]]\n\n# example for median i.e. 50% percentile\np_fns$`50`(simdat, \"S\")\n\n[1] 4.947945\n\ne_times <- purrr::map_dbl(p_fns, ~ do.call(.x, list(simdat, \"S\")))\nupp_times <- purrr::map_dbl(p_fns, ~ do.call(.x, list(simdat, \"upp\")))\nlow_times <- purrr::map_dbl(p_fns, ~ do.call(.x, list(simdat, \"low\")))\n\nWe can plot all of the percentile times with error bounds as follows.\n\nplot(mle) + \n  geom_vline(xintercept = e_times) +\n  geom_vline(xintercept = upp_times, linetype = 2) +\n  geom_vline(xintercept = low_times, linetype = 2) +\n  annotate(\"text\", x = e_times + 0.5, y = 0.25, label = prctile)\n\n\n\n\n\n\nComparing between all distribution fits and Kaplan-Meier\nIn this section we bring together various things from previous sections. We will do an analysis for all 6 main distributions at the same time and for several percentiles.\nFirst, we fit all of the models and then generate the sample of survival curves.\n\ndist_names <- c(\"exponential\", \"weibull\", \"gompertz\", \"loglogistic\", \"lognormal\", \"gengamma\")\n\nmle <- fit.models(formula = Surv(recyrs, censrec) ~ 1,\n                  data = bc,\n                  distr = dist_names,\n                  method = \"mle\")\n\nsurv <- purrr::map(setNames(1:6, dist_names), ~ make.surv(mle, mod = .x, nsim = NSIM))\n\nNow, for each distribution we calculate the survival times at each chosen percentile.\n\ntimes <- list()\n\nfor (i in dist_names) {\n  simdat <- surv[[i]]$S[[1]]\n  times[[i]] <- purrr::map_dbl(p_fns, ~ do.call(.x, list(simdat, \"S\")))\n}\n\nFinally, we can plot the results, including the Kaplan-Meier estimates.\n\nlibrary(scales)\n\n## ggplot2 default colours\ncols <- hue_pal()(6)\nkm_dat <- mle$misc$km\n  \nt_km <- purrr::map_dbl(prctile, ~min(km_dat$time[km_dat$surv < .x]))\n\nplot(mle) + \n  purrr::map(seq_along(times), ~ geom_vline(xintercept = times[[.x]], col = cols[.x])) +\n  geom_vline(xintercept = t_km, size = 1.5, linetype = 2)\n\n\n\n\nWe haven’t included the upper and lower bound here because the plot would be too busy but it is trivial to extend the code above to do this.\nLet us create a table of these percentile outputs too.\n\ntab <- t(do.call(rbind, times))\ntab <- cbind(tab, Observed = t_km)\n\nknitr::kable(round(tab, 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexponential\nweibull\ngompertz\nloglogistic\nlognormal\ngengamma\nObserved\n\n\n\n\n97.5\n0.18\n0.41\n0.27\n0.44\n0.50\n0.58\n0.56\n\n\n75\n2.03\n2.34\n2.18\n2.19\n2.17\n2.03\n1.99\n\n\n50\n4.90\n4.66\n4.80\n4.50\n4.66\n4.81\n4.95\n\n\n25\nInf\nInf\nInf\nInf\nInf\nInf\nInf\n\n\n2.5\nInf\nInf\nInf\nInf\nInf\nInf\nInf\n\n\n\n\n\n\n\nSurvival probabilities at given times\nWe can flip the analysis around and instead obtain survival probabilities at user-defined time points.\nThe code looks veery similar to the percentile case above.\n\nt_pt <- c(1,2,5)\n\nS_est <- list()\n\nfor (i in dist_names) {\n  simdat <- surv[[i]]$S[[1]]\n  S_est[[i]] <- purrr::map_dbl(t_pt, ~min(simdat$S[simdat$t < .x]))\n}\n\n\nkm_dat <- mle$misc$km\nt_km <- purrr::map_dbl(t_pt, ~min(km_dat$surv[km_dat$time < .x]))\n\nplot(mle) + \n  purrr::map(seq_along(S_est), ~ geom_hline(yintercept = S_est[[.x]], col = cols[.x])) +\n  geom_vline(xintercept = t_pt) +\n  geom_hline(yintercept = t_km, size = 1.5, linetype = 2)\n\n\n\n\nLet us create a table of these survival probabilities as percentages.\n\ntab <- t(do.call(rbind, S_est))\ntab <- cbind(time = t_pt, tab*100, Observed = t_km*100)\n\nknitr::kable(round(tab, 0))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime\nexponential\nweibull\ngompertz\nloglogistic\nlognormal\ngengamma\nObserved\n\n\n\n\n1\n87\n91\n88\n91\n91\n91\n92\n\n\n2\n75\n79\n77\n78\n77\n75\n75\n\n\n5\n49\n47\n48\n46\n47\n49\n49"
  },
  {
    "objectID": "other-hybrid-methods.html",
    "href": "other-hybrid-methods.html",
    "title": "Other hybrid methods",
    "section": "",
    "text": "References\n\nGelber, Richard D., Aron Goldhirsch, and Bernard F. Cole. 1993. “Parametric extrapolation of survival estimates with applications to quality of life evaluation of treatments.” Control. Clin. Trials 14 (6): 485–99. https://doi.org/10.1016/0197-2456(93)90029-D."
  },
  {
    "objectID": "other-methods.html",
    "href": "other-methods.html",
    "title": "Other methods",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "parametric-modelling.html",
    "href": "parametric-modelling.html",
    "title": "Parametric modelling",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "PH-modelling.html",
    "href": "PH-modelling.html",
    "title": "Proportional Hazard Modelling",
    "section": "",
    "text": "Goeman, J., Meijer, R. and Chaturvedi, N. (2013) L 1 (lasso and fused lasso) and L 2 (ridge) penalized estimation in GLMs and in the Cox model. R package version 0.9-42"
  },
  {
    "objectID": "restricted-means.html",
    "href": "restricted-means.html",
    "title": "Restricted Means",
    "section": "",
    "text": "library(survRM2)"
  }
]