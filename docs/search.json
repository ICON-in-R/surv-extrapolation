[
  {
    "objectID": "AIC-BIC-tests.html",
    "href": "AIC-BIC-tests.html",
    "title": "AIC/BIC tests",
    "section": "",
    "text": "Akaike’s Information Criterion (AIC) and the Bayesian Information Criterion (BIC) provide a useful statistical test of the relative fit of alternative parametric models, and they are usually available as outputs from statistical software. Further details on these are available from (Collett 2013). Measures such as the negative 2 log likelihood are only suitable for comparing nested models, whereby one model is nested within another (for example, one model adds an additional covariate compared to another model). Different parametric models which use different probability distributions cannot be nested within one another. Thus the negative 2 log likelihood test is not suitable for assessing the fit of alternative parametric models. The AIC and BIC allow a comparison of models that do not have to be nested, including a term which penalises the use of unnecessary covariates (these are penalised more highly by the BIC). Generally, it is not necessary to include covariates in survival modelling in the context of an RCT as it would be expected that any important covariates would be balanced through the process of randomisation. However, some parametric models have more parameters than others, and the AIC and BIC take account of these – for example an exponential model only has one parameter and so in comparative terms two parameter models such as the Weibull or Gompertz models are penalised. The AIC and BIC statistics therefore weigh up the improved fit of models with the potentially inefficient use of additional parameters, with the use of additional parameters penalised more highly by the BIC relative to the AIC.\nSuppose that we have a statistical model of some data. Let \\(k\\) be the number of estimated parameters in the model. Let \\(\\hat{L}\\) be the maximized value of the likelihood function for the model. Then the AIC value of the model is the following.\n\\[\nAIC = 2k - 2 \\ln({\\hat{L}})\n\\]\nGiven a set of candidate models for the data, the preferred model is the one with the minimum AIC value.\nThe BIC is formally defined as\n\\[\nBIC = k \\ln(n) - 2 \\ln(\\widehat{L})\n\\]\nwhere\n\\(\\hat{L}\\) = the maximized value of the likelihood function of the model \\(M\\), i.e. \\(\\hat{L} = p(x \\mid \\widehat{\\theta}, M)\\), where \\(\\widehat{\\theta}\\) are the parameter values that maximize the likelihood function; \\(x\\) = the observed data; \\(n\\) = the number of data points in \\(x\\), the number of observations, or equivalently, the sample size; \\(k\\) = the number of parameters estimated by the model. For example, in multiple linear regression, the estimated parameters are the intercept, the \\(q\\) slope parameters, and the constant variance of the errors; thus, \\(k = q + 2\\).\n\n\n\n\n\n\nRelative values\n\n\n\nIC values only really make sense when compared between one another i.e relatively. The absolute value is not helpful and depends on the specifics of the model and data."
  },
  {
    "objectID": "AIC-BIC-tests.html#r-example",
    "href": "AIC-BIC-tests.html#r-example",
    "title": "AIC/BIC tests",
    "section": "R example",
    "text": "R example\nCox proportional hazard model\n\nlibrary(survival)\n\ntest1 <- list(time=c(4,3,1,1,2,2,3), \n              status=c(1,1,1,0,1,1,0), \n              x=c(0,2,1,1,1,0,0), \n              sex=c(0,0,0,0,1,1,1)) \n# Fit a stratified model \nfit_ph <- coxph(Surv(time, status) ~ x + strata(sex), test1) \n\nAIC(fit_ph)\n\n[1] 8.655313\n\n##TODO: what this difference with AIC()?\nextractAIC(fit_ph)\n\n[1] 1.000000 8.655313\n\nBIC(fit_ph)\n\n[1] 8.264751\n\n\n\nParametric model with flexsurv\nThe package flexsurv computes the AIC when it fits a model. This can be accessed by `.\\(AIC\\) i.e.\n\nlibrary(flexsurv)\n\nfitg <- flexsurvreg(formula = Surv(futime, fustat) ~ 1, data = ovarian, dist = \"gengamma\")\nfitg$AIC\n\n[1] 199.8981\n\n\nFor other information criteria statistics it is straightforward to calculate these using the flexsurvreg outpuu. For simplicity we can write a function to do them all at the same time, which we shall call fitstats.flexsurvreg.\n\n\nfitstats.flexsurvreg() function\n# helper function\n# from flexsurv github\n# https://github.com/chjackson/flexsurv-dev/issues/44\nfitstats.flexsurvreg <- function(x) {\n  ll <- x$loglik\n  aic <- x$AIC\n  k <- length(x$coefficients)\n  n <- sum(x$data$m[\"(weights)\"])\n  aicc <- aic + ((2 * k) * (k + 1) / (n - k - 1))\n  bic <- - 2 * ll + (k * log(n))\n  \n  data.frame(\n   Df = k,\n    \"n2ll\" = -2 * ll,\n    AIC = aic,\n    AICc = aicc,\n    BIC = bic)\n}\n\n\nNow if we run this we obtain all of the statistics for the previous generalised gamma model fit.\n\nfitstats.flexsurvreg(fitg)\n\n  Df     n2ll      AIC    AICc      BIC\n1  3 193.8981 199.8981 200.989 203.6724\n\n\n\n\nBayesian model with survHE package\nFirst let us fit an example model using survHE.\n\nlibrary(survHE)\n\ndata(bc)\n\n# Fits the same model using the 3 inference methods\nmle <- fit.models(formula = Surv(recyrs,censrec) ~ group,\n                  data = bc,\n                  distr = \"exp\",\n                  method = \"mle\")\n\nNow, the print method for class survHE returns several model fitting summaries.\n\nprint(mle)\n\n\nModel fit for the Exponential model, obtained using Flexsurvreg \n(Maximum Likelihood Estimate). Running time: 0.020 seconds\n\n                 mean         se      L95%      U95%\nrate        0.0603838 0.00845542 0.0458911 0.0794534\ngroupMedium 0.8180219 0.17122084 0.4824352 1.1536086\ngroupPoor   1.5375232 0.16280169 1.2184378 1.8566087\n\nModel fitting summaries\nAkaike Information Criterion (AIC)....: 1668.212\nBayesian Information Criterion (BIC)..: 1681.805\n\n\nIf we wished to access the values directly, perhaps to use in our own code, then we can use\n\nmle$model.fitting\n\n$aic\n[1] 1668.212\n\n$bic\n[1] 1681.805\n\n$dic\nNULL\n\n\nFurther, if we were to fit several different models to compare the IC statistics, which is really the main point of doing it, then survHE also has some nice plotting functions.\n\nmle_multi <- fit.models(formula = Surv(recyrs,censrec) ~ group,\n                  data = bc,\n                  distr = c(\"exp\", \"weibull\", \"gompertz\", \"lognormal\", \"loglogistic\"),\n                  method = \"mle\")\n\nmodel.fit.plot(mle_multi)\n\n\n\nmodel.fit.plot(mle_multi, type = \"BIC\")\n\n\n\nmodel.fit.plot(mle_multi, scale = \"relative\")"
  },
  {
    "objectID": "assess-transformed-km.html",
    "href": "assess-transformed-km.html",
    "title": "Assessing model assumptions using transformed hazard plots",
    "section": "",
    "text": "Prior to fitting a model based on an assumed parametric form for the hazard function, a preliminary study of the validity of this assumption should be carried-out.\nLet us compare the survivor function for the data with that from a chosen model. To do this we will transform the survivor function to produce a plot that should give a straight line if the assumed model is appropriate.\nFor the Weibull, twice taking logs of the survivor function with scale parameter \\(\\lambda\\) and shape parameter \\(\\gamma\\)\n\\[\nlog(-log S(t)) = log \\lambda + \\gamma log t\n\\]\nA plot of \\(log(-log S(t))\\) against \\(log(t)\\) would give an approximately straight line if the Weibull assumption is reasonable. The plot could also be used to give a rough estimate of the parameters.\nSimilarly, for the log-logistic distribution\n\\[\nlog S(t)/(1 - S(t)) = \\theta - \\kappa log t\n\\]\nFor the log-normal distribution\n\\[\n\\Phi^{-1} (1 - S(t)) = (log t - \\mu) / \\sigma\n\\] The slope and intercept of this line provide estimates of \\(\\sigma^{-1}\\) and \\(-\\mu/\\sigma\\), respectively.\nWe can also check the assumption made with using the Cox regression model of proportional hazards by inspecting the log-cumulative hazard plot.\n\\[\nlog H_i(t) = \\beta x_i + log H_0(t)\n\\]\nThe transformed curves for different values of the explanatory variables will be parallel if PH holds.\nSee Collett (2013) for more details."
  },
  {
    "objectID": "assess-transformed-km.html#r-examples",
    "href": "assess-transformed-km.html#r-examples",
    "title": "Assessing model assumptions using transformed hazard plots",
    "section": "R examples",
    "text": "R examples\nThe package commonly used for survival analyses in R is the survival package (Therneau T 2021). We will begin by repeating an example from the survival help documentation.\nThis uses their reliability data. Firstly a little data manipulation is done before we plot the cumulative hazard plot against time using the in-built survival package plotting method with the cumhaz=TRUE argument.\n\nlibrary(survival)\n\ndata(\"reliability\", package = \"survival\")\n\nvdata <- with(valveSeat, data.frame(id = id, time2 = time, status = status))\nfirst <- !duplicated(vdata$id)\nvdata$time1 <- ifelse(first, 0, c(0, vdata$time[-nrow(vdata)]))\ndouble <- which(vdata$time1 == vdata$time2)\nvdata$time1[double] <- vdata$time1[double] - 0.01\nvdata$time2[double - 1] <- vdata$time1[double]\nvdata[1:7, c(\"id\", \"time1\", \"time2\", \"status\")]\n\n   id  time1  time2 status\n1 251   0.00 761.00      0\n2 252   0.00 759.00      0\n3 327   0.00  98.00      1\n4 327  98.00 667.00      0\n5 328   0.00 326.00      1\n6 328 326.00 652.99      1\n7 328 652.99 653.00      1\n\n\n\nfit <- survfit(Surv(time1, time2, status) ~ 1, data = vdata, id = id)\nplot(fit, cumhaz = TRUE, xlab = \"Days\", ylab = \"Cumulative hazard\")\n\n\n\n\nWe can plot the log-cumulative hazard against log-time by simply plotting the survfit output values directly by specifying the x and y data explicitly.\n\nplot(log(fit$time), log(fit$cumhaz), xlab = \"log-Days\", ylab = \"Log-cumulative hazard\", type = \"l\")\n\n\n\n\nFor the following, the latest development version fo the survHE package (Baio 2020) contains all of the functions that we will need. We can obtain this from GitHub with the following.\n\ndevtools::install_github(\"giabaio/survHE\", ref = \"devel\")\n\nIn particular, we will need the plot_transformed_km function.\n\n\nplot_transformed_km() function\nplot_transformed_km <- function(fit, mod = 1, add_legend = FALSE,\n                                graph = \"base\", ...) {\n  \n  dots <- list(...)\n  \n  graph <- match.arg(graph, c(\"base\", \"ggplot2\"))\n  \n  if (length(mod) != 1)\n    stop(\"Please provide at most one model index.\")\n  \n  if (is.numeric(mod) && !mod <= length(fit$models))\n    stop(\"More model names provided than available in list of model fits provided.\")\n  \n  if (is.character(mod) && !mod %in% names(fit$models))\n    stop(\"Model name not available in list of model fits provided.\")\n  \n  dist <- get_distribution(fit, mod)\n  \n  distn_names <- list(\n    \"exp\" = c(\"exp\", \"exponential\"),\n    \"weibull\" = c(\"weibull\", \"weibull.quiet\", \"weibullaf\", \"weibullph\"),\n    \"loglogistic\" = c(\"llogis\", \"loglogistic\"),\n    \"lognormal\" = c(\"lognormal\", \"lnorm\"),\n    \"gompertz\" = \"gompertz\")\n  \n  if (!dist %in% unname(unlist(distn_names)))\n    stop(\"Distribution not available.\")\n  \n  fit_km <- fit$misc$km\n  \n  n_strata <- length(fit_km$strata)\n  \n  if (n_strata == 0 || n_strata == 1) {\n    fit_km$strata <- c(\"group\" = length(fit_km$time))\n  }\n  \n  model_strata <- rep(x = names(fit_km$strata),\n                      times = fit_km$strata)\n  \n  times <- split(fit_km$time, model_strata)\n  survs <- split(fit_km$surv, model_strata)\n  \n  params <- list()\n  \n  if (dist %in% distn_names[[\"exp\"]]) {\n    params <- list(\n      FUN = \"lines\",\n      xlab = \"time\",\n      ylab = \"-log(S(t))\",\n      main = \"Exponential distributional assumption\",\n      x = times,\n      y = lapply(survs, function(x) -log(x)),\n      lty = 1:n_strata,\n      col = 1:n_strata,\n      type = \"l\")\n  }\n  \n  if (dist %in% distn_names[[\"weibull\"]]) {\n    params <- list(\n      FUN = \"lines\",\n      xlab = \"log(time)\",\n      ylab = \"log(-log(S(t))) i.e. log cumulative hazard\",\n      main = \"Weibull distributional assumption\",\n      x = lapply(times, log),\n      y = lapply(survs, function(x) log(-log(x))),\n      lty = 1:n_strata,\n      col = 1:n_strata,\n      type = \"l\")\n  }\n  \n  if (dist %in% distn_names[[\"loglogistic\"]]) {\n    params <- list(\n      FUN = \"lines\",\n      xlab = \"time\",\n      ylab = \"log(S(t)/(1-S(t)))\",\n      main = \"log-Logistic distributional assumption\",\n      x = lapply(times, log),\n      y = lapply(survs, function(x) log(x/(1 - x))),\n      lty = 1:n_strata,\n      col = 1:n_strata,\n      type = \"l\")\n  }\n  \n  if (dist %in% distn_names[[\"lognormal\"]]) {\n    params <- list(\n      FUN = \"lines\",\n      xlab = \"log(time)\",\n      ylab = expression(Phi^-1 ~ (1 - S(t))),\n      main = \"Log-normal distributional assumption\",\n      x = lapply(times, log),\n      y = lapply(survs, function(x) qnorm(1 - x)),\n      lty = 1:n_strata,\n      col = 1:n_strata,\n      type = \"l\")\n  }\n  \n  default_pars <- list(\n    x = NULL,\n    type = \"n\",\n    axes = FALSE,\n    xlab = params$xlab,\n    ylab = params$ylab,\n    main = params$main,\n    xlim = range(pretty(unlist(params$x))),\n    ylim = range(pretty(unlist(params$y))))\n  \n  setup_pars <- modifyList(\n    default_pars, dots[names(default_pars)])\n  \n  if (graph == \"base\") {\n    \n    # empty plot\n    do.call(plot, setup_pars)\n    \n    axis(1); axis(2)\n    \n    # plot lines\n    do.call(mapply, modifyList(params, dots))\n    \n    if (isTRUE(add_legend)) {\n      legend(\"topright\", names(survs), col = params$col,\n             lty = params$lty, bty = \"n\")\n    }\n  }\n  \n  if (graph == \"ggplot2\") {\n    \n    if (!add_legend) {\n      pos.legend <- \"none\"\n    } else {\n      pos.legend <- \"right\"}\n    \n    ggdata <- \n      data.frame(time = unlist(params$x),\n                 y = unlist(params$y)) |>\n      tibble::rownames_to_column(\"Group\") |> \n      mutate(Group = gsub(\"\\\\d+\", \"\", Group))\n    \n    p <- \n      ggplot(ggdata, aes(x = .data$time, y = .data$y,\n                         group = .data$Group, col = .data$Group)) +\n      geom_line() +\n      do.call(labs,\n              list(title = setup_pars$main,\n                   x = setup_pars$xlab,\n                   y = setup_pars$ylab)) +\n      theme_bw() +\n      theme(legend.position = pos.legend)\n    \n    print(p)\n  }\n  \n  invisible(params)\n}\n\nget_distribution <- function(fit, mod) {\n    m <- fit$models[[mod]]\n    tolower(ifelse(fit$method == \"hmc\", m@model_name, m$dlist$name))\n}\n\n\nNow we can repeat the above analysis using the plot_transformed_km function. By setting distr = \"exp\" the cumulative hazard plot is returned.\n\nlibrary(survHE)\n\nLoading required package: flexsurv\n\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: ggplot2\n\nfit_exp <- survHE::fit.models(Surv(time1, time2, status) ~ 1,\n                              data = vdata, distr = \"exp\", method = \"mle\")\nplot_transformed_km(fit_exp)\n\n\n\n\nSetting distr = \"weibull\" then we get the log-cumulative hazard against log-time plot.\n\nfit_wei <- survHE::fit.models(Surv(time1, time2, status) ~ 1,\n                              data = vdata, distr = \"weibull\", method = \"mle\")\nplot_transformed_km(fit_wei)\n\n\n\n\nThe plot_transformed_km also provides plots for log-normal and log-logistic distribution assumptions with the corresponding transformation to the survival data.\n\nUsing flexsurv\nFurther, we could use the flexsurv package (Jackson 2016). This package contains lots of functions for a range of survival distributions.\nThe cumulative hazard can be plotted with the flexsurv plotting method with argument type = \"cumhaz\". The Kaplan-Meier is also overlaid by the model fit.\n\nlibrary(\"flexsurv\")\n\nfs1 <- flexsurvreg(Surv(time1, time2, status) ~ 1, data = vdata, dist = \"exp\")\nplot(fs1, type = \"cumhaz\")\n\n\n\nfs2 <- flexsurvreg(Surv(time1, time2, status) ~ 1, data = vdata, dist = \"weibull\")\nplot(fs2, type = \"cumhaz\")\n\n\n\n\n\n\nUsing survHE\n\nlibrary(\"survHE\")\n\nfs1 <- fit.models(Surv(time1, time2, status) ~ 1, data = vdata, dist = \"exp\")\nplot(fs1, type = \"cumhaz\")\n\n\n\nfs2 <- fit.models(Surv(time1, time2, status) ~ 1, data = vdata, dist = \"weibull\")\nplot(fs2, type = \"cumhaz\")"
  },
  {
    "objectID": "basic-visual-inspection.html",
    "href": "basic-visual-inspection.html",
    "title": "Basic visual inspection",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blended-curves.html",
    "href": "blended-curves.html",
    "title": "Blended survival curves",
    "section": "",
    "text": "We now present a novel approach to alleviate the problem of survival extrapolation with heavily censored data from clinical trials. The main idea is to mix a flexible model (e.g., Cox semiparametric) to fit as well as possible the observed data and a parametric model encoding assumptions on the expected behaviour of underlying long-term survival. The two are ‘’blended’’ into a single survival curve that is identical with the Cox model over the range of observed times and gradually approaching the parametric model over the extrapolation period based on a weight function. The weight function regulates the way two survival curves are blended, determining how the internal and external sources contribute to the estimated survival over time. Further details can be found in (Che, Green, and Baio 2022).\n\n\n\n\n\n\nShiny app\n\n\n\nThere is an RShiny version of the blendR package for using blended curves interactively in the browser. See here."
  },
  {
    "objectID": "blended-curves.html#r-examples",
    "href": "blended-curves.html#r-examples",
    "title": "Blended survival curves",
    "section": "R Examples",
    "text": "R Examples\nWe need to have the blendR package installed to run this example. This is currently available on GitHub.\n\ndevtools::install_github(\"StatisticsHealthEconomics/blendR\")\n\nIn the first example we will use the survHE and INLA packages to fit the external and observed data models, respectively. If the survHE version for doing HMC is missing then install this.\n\nremotes::install_github('giabaio/survHE', ref='hmc')\n\nAttach these packages.\n\n\n\nWe will use the data set available within blendR and so load data in to the current environment.\n\ndata(\"TA174_FCR\", package = \"blendR\")\nhead(dat_FCR)\n\n# A tibble: 6 × 5\n  patid treat death death_t death_ty\n  <int> <int> <int>   <dbl>    <dbl>\n1     1     1     0  32       2.67  \n2     2     1     0  30.6     2.55  \n3     3     1     0  28       2.33  \n4     8     1     0  30       2.5   \n5    10     1     1   0.458   0.0382\n6    11     1     1   1.57    0.131 \n\n\nFit to the observed data uinsg INLA to obtain the survival object. blendR has a helper function to do this for a piece-wise exponential distribution. The cutpoints argument determines where the points on the survival curve are between which the hazard is constant i.e. an exponential curve.\n\nobs_Surv <- blendR::surv_est_inla(data = dat_FCR,\n                                  cutpoints = seq(0, 180, by = 5))\n\nSimilarly, we fit the external estimate but first we need to create a synthetic data set consistent with expert judgment. This can be elicited ad-hoc or formally and the process of doing so is a field in itself. Once the values have been elicited then blendR had a function to translate from elicited survival curve constraints to a random sample of survival times. In this case we suppose that we have the information that at time 144 the probability of survival is 0.05.\n\ndata_sim <- blendR::ext_surv_sim(t_info = 144,\n                                  S_info = 0.05,\n                                  T_max = 180)\n\next_Surv <- fit.models(formula = Surv(time, event) ~ 1,\n                       data = data_sim,\n                       distr = \"gompertz\",\n                       method = \"hmc\",\n                       priors = list(gom = list(a_alpha = 0.1,\n                                                b_alpha = 0.1)))\n\n\nSAMPLING FOR MODEL 'Gompertz' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.231 seconds (Warm-up)\nChain 1:                0.217 seconds (Sampling)\nChain 1:                0.448 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'Gompertz' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.621 seconds (Warm-up)\nChain 2:                0.212 seconds (Sampling)\nChain 2:                0.833 seconds (Total)\nChain 2: \n\n\nNow we are nearly ready to fit the blended survival curve. We also need to provide the additional information of how the observed data and external curves are blended together using the beta distribution. That is, we define the blending region min and max and the parameters alpha and beta.\n\nblend_interv <- list(min = 48, max = 150)\nbeta_params <- list(alpha = 3, beta = 3)\n\nbefore putting this all together in the blendsurv function.\n\nble_Surv <- blendR::blendsurv(obs_Surv, ext_Surv, blend_interv, beta_params)\n\nA plotting method is available for blendR objects so simply call the following to return the blended survival curve graph.\n\nplot(ble_Surv)\n\n\n\n\nWe can alternatively use other survival curves and fitting function for each part of the blended curve. Here we use also fit.model from survHE instead of the INLA fitting function for the observed data model.\n\nobs_Surv2 <- fit.models(formula = Surv(death_t, death) ~ 1,\n                        data = dat_FCR,\n                        distr = \"exponential\",\n                        method = \"hmc\")\n\n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.402 seconds (Warm-up)\nChain 1:                0.276 seconds (Sampling)\nChain 1:                0.678 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.4 seconds (Warm-up)\nChain 2:                0.267 seconds (Sampling)\nChain 2:                0.667 seconds (Total)\nChain 2: \n\next_Surv2 <- fit.models(formula = Surv(time, event) ~ 1,\n                        data = data_sim,\n                        distr = \"exponential\",\n                        method = \"hmc\")\n\n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.142 seconds (Warm-up)\nChain 1:                0.081 seconds (Sampling)\nChain 1:                0.223 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'Exponential' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.128 seconds (Warm-up)\nChain 2:                0.057 seconds (Sampling)\nChain 2:                0.185 seconds (Total)\nChain 2: \n\nble_Surv2 <- blendR::blendsurv(obs_Surv2, ext_Surv2, blend_interv, beta_params)\n\nWe can also include the original data Kaplan-Meier in the output plot by simply appending it to the basic plot.\n\n# kaplan-meier\nkm <- survfit(Surv(death_t, death) ~ 1, data = dat_FCR)\n\nplot(ble_Surv2) +\n  geom_line(aes(km$time, km$surv, colour = \"Kaplan-Meier\"),\n            size = 1.25, linetype = \"dashed\")"
  },
  {
    "objectID": "external-data.html",
    "href": "external-data.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "surv-extrapolation",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro-to-R.html",
    "href": "intro-to-R.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "\\[\n\\mbox{E}[T \\mid \\boldsymbol{\\theta}]= \\int_0^\\infty S(t)dt.\n\\]\nThis implies that the expected (i.e. mean) survival time can be computed as the “area under the survival curve”. More importantly, this clarifies why applications of survival modelling in HTA requires a full extrapolation of the survival curve: the focus of the economic analysis is on the mean survival time and in order to obtain it, we need to be able to derive the full survival curve in the range of times \\(t = 0, \\ldots, \\infty\\).\nA challenge with estimating lifetime survival based on trial data is that we can only observe death times \\(T\\) that are shorter than the follow-up time of the trial. For people still alive at the end of the trial, the survival times are right-censored. To estimate \\(E(T)\\), we need to know the distribution of \\(T\\) over all plausible values of \\(T\\)."
  },
  {
    "objectID": "medians.html",
    "href": "medians.html",
    "title": "Medians",
    "section": "",
    "text": "Tip"
  },
  {
    "objectID": "medians.html#r-examples",
    "href": "medians.html#r-examples",
    "title": "Medians",
    "section": "R Examples",
    "text": "R Examples\nThe summary method for a survHE object returns mean survival times, including the median mean survival time.\n\nlibrary(survHE)\n\ndata(bc)\n\nmle <- fit.models(formula = Surv(recyrs, censrec) ~ 1,\n                  data = bc,\n                  distr = \"exp\",\n                  method = \"mle\")\n\nsummary(mle)\n\n\nEstimated average survival time distribution* \n     mean        sd     2.5%  median    97.5%\n 4.522619 0.1142604 4.285068 4.52252 4.742877\n\n*Computed over the range: [0.02192-7.28493] using 1000 simulations.\nNB: Check that the survival curves tend to 0 over this range!\n\n\nWe can compare these parametric estimate with the median survival time from the Kaplan-Meier.\n\nmin(mle$misc$km$time[mle$misc$km$surv < 0.5])\n\n[1] 4.950685\n\n\nIf we denote the median with \\(t_{50}\\) then to get the medians ourselves we can take the coefficient value from the fit.model output and use the fact that\n\\[\nt_{50} = - log (0.5)/\\lambda\n\\]\n\nrate <- mle$models$Exponential$coefficients\nexp(rate)\n\n[1] 0.1414765\n\n# closed form\nmeantime <- -log(0.5)/exp(rate)\nmeantime\n\n[1] 4.899379\n\n\nNote that the parameter returned from fit.model is the log of the rate. Alternatively, and more general, we can simulate (multiple) survival curves from the coefficient posterior and estimate the median for each of these.\nSo, sample from the posterior using make.surv() from the survHE package to get for the single curve case as follows\n\nxx <- make.surv(mle)\nmin(xx$S[[1]]$t[xx$S[[1]]$S < 0.5])\n\n[1] 4.893151\n\n\nIt follows that we can do something similar for multiple simulations to obtain uncertainty bounds.\n\nxx <- make.surv(mle, nsim = 100)\n\n# direct estimates\nrtimes <- -log(0.5)/unlist(xx$sim)\n\n# simulated estimates\nt_S <- min(xx$S[[1]]$t[xx$S[[1]]$S < 0.5])\nt_low <- min(xx$S[[1]]$t[xx$S[[1]]$low < 0.5])\nt_upp <- min(xx$S[[1]]$t[xx$S[[1]]$upp < 0.5])\n\n\nplot(mle) + \n  geom_vline(xintercept = rtimes, alpha = 0.1, col = \"darkgrey\", size = 2) +\n  geom_vline(xintercept = meantime) +\n  geom_vline(xintercept = t_low, linetype = 2) +\n  geom_vline(xintercept = t_upp, linetype = 2)\n\n\n\n\n\nMultiple distributions\n\nfit2 <- fit.models(formula = Surv(recyrs, censrec) ~ 1,\n                   data = bc,\n                   dist = c(\"exp\", \"loglogistic\"),\n                   method = \"mle\")\n\nplot(fit2)\n\n\n\n\nThe log-logistic distribution has CDF\n\\[\n\\frac{1}{(1 + (t/\\alpha)^{\\beta})^2}\n\\]\nWhich leads to the median \\(t_{50} = \\alpha\\), the shape parameter.\n\nsim <- list()\nsim[[1]] <- make.surv(fit2, mod = 1, nsim = 100)\nsim[[2]] <- make.surv(fit2, mod = 2, nsim = 100)\n\nsim <- purrr::transpose(sim)\n\n# direct estimates\nrtimes <- list()\nrtimes[[1]] <- -log(0.5)/sim$sim[[1]][[1]][, \"rate\"]\nrtimes[[2]] <- sim$sim[[2]][[1]][, \"scale\"]\n\n# simulated estimates\nt_S <- purrr::map_dbl(sim$S, ~ min(.x[[1]]$t[.x[[1]]$S < 0.5]))\nt_low <- purrr::map_dbl(sim$S, ~ min(.x[[1]]$t[.x[[1]]$low < 0.5]))\nt_upp <- purrr::map_dbl(sim$S, ~ min(.x[[1]]$t[.x[[1]]$upp < 0.5]))\n\n\nplot(fit2) + \n  geom_vline(xintercept = rtimes[[1]], alpha = 0.1, col = \"pink\", size = 2) +\n  geom_vline(xintercept = rtimes[[2]], alpha = 0.1, col = \"lightblue\", size = 2) +\n  geom_vline(xintercept = t_S[[1]]) +\n  geom_vline(xintercept = t_low[[1]], linetype = 2) +\n  geom_vline(xintercept = t_upp[[1]], linetype = 2) +\n  geom_vline(xintercept = t_S[[2]]) +\n  geom_vline(xintercept = t_low[[2]], linetype = 3) +\n  geom_vline(xintercept = t_upp[[2]], linetype = 3)\n\n\n\n\n\n\nMultiple percentiles\nhttps://stats.stackexchange.com/questions/19005/finding-median-survival-time-from-survival-function#:~:text=M%3D%7Bt(n%2B,S(M)%3D0.5.\n\n\nCompare with Kaplan-Meier\n\n\nMultiple groups\n\n# Fits the same model using the 3 inference methods\nmle <- fit.models(formula = Surv(recyrs,censrec) ~ group,\n                  data = bc,\n                  distr = \"exp\",\n                  method = \"mle\")\nmle\n\n\nModel fit for the Exponential model, obtained using Flexsurvreg \n(Maximum Likelihood Estimate). Running time: 0.010 seconds\n\n                 mean         se      L95%      U95%\nrate        0.0603838 0.00845542 0.0458911 0.0794534\ngroupMedium 0.8180219 0.17122084 0.4824352 1.1536086\ngroupPoor   1.5375232 0.16280169 1.2184378 1.8566087\n\nModel fitting summaries\nAkaike Information Criterion (AIC)....: 1668.212\nBayesian Information Criterion (BIC)..: 1681.805\n\nxx <- make.surv(mle)\n\npurrr::map_dbl(xx$S, ~ min(.x$t[.x$S < 0.5]))\n\nWarning in min(.x$t[.x$S < 0.5]): no non-missing arguments to min; returning Inf\n\n\n[1]      Inf 5.043836 2.509589\n\nrate <- mle$models$Exponential$coefficients\n\nmeantime <- -log(0.5)/rate\n\n\n## plots"
  },
  {
    "objectID": "other-hybrid-methods.html",
    "href": "other-hybrid-methods.html",
    "title": "Other hybrid methods",
    "section": "",
    "text": "References\n\nGelber, Richard D., Aron Goldhirsch, and Bernard F. Cole. 1993. “Parametric extrapolation of survival estimates with applications to quality of life evaluation of treatments.” Control. Clin. Trials 14 (6): 485–99. https://doi.org/10.1016/0197-2456(93)90029-D."
  },
  {
    "objectID": "other-methods.html",
    "href": "other-methods.html",
    "title": "Other methods",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "parametric-modelling.html",
    "href": "parametric-modelling.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "PH-modelling.html",
    "href": "PH-modelling.html",
    "title": "Proportional Hazard Modelling",
    "section": "",
    "text": "Goeman, J., Meijer, R. and Chaturvedi, N. (2013) L 1 (lasso and fused lasso) and L 2 (ridge) penalized estimation in GLMs and in the Cox model. R package version 0.9-42"
  },
  {
    "objectID": "restricted-means.html",
    "href": "restricted-means.html",
    "title": "Restricted Means",
    "section": "",
    "text": "library(survRM2)\n\nWarning: package 'survRM2' was built under R version 4.1.3\n\n\nLoading required package: survival"
  }
]